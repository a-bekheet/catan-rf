# Multi-Agent RL Training System

## ğŸ¯ Overview

This system enables training AI agents using three cutting-edge RL frameworks to play Settlers of Catan:

1. **Ray RLlib (PPO)** - Proximal Policy Optimization
2. **TorchRL (SAC)** - Soft Actor-Critic
3. **LangGraph (LLM)** - LLM-powered strategic reasoning

## ğŸš€ Quick Start

### Option 1: Interactive Menu
```bash
python run.py
# Select option 2: Multi-Agent RL Training
```

### Option 2: Direct Training
```bash
# Train a specific agent
python train_agents.py --agent ppo --episodes 100
python train_agents.py --agent sac --episodes 100
python train_agents.py --agent llm --episodes 100

# Train all agents + tournament
python train_agents.py --agent all --episodes 100 --tournament 50
```

### Option 3: Interactive Training CLI
```bash
python train_agents.py
# Follow the interactive prompts
```

## ğŸ“¦ Installation

### Install Dependencies
```bash
pip install -r requirements.txt
```

### Optional: LLM API Keys (for LangGraph agent)

Set environment variables for LLM providers:

```bash
# OpenAI (default)
export OPENAI_API_KEY="your-key-here"

# or Anthropic Claude
export ANTHROPIC_API_KEY="your-key-here"
```

**Note**: The LLM agent will work without API keys using heuristic fallback mode.

## ğŸ¤– Agent Types

### 1. Ray RLlib PPO Agent

**Algorithm**: Proximal Policy Optimization
**Best For**: Fast distributed training, stable policy updates
**Key Features**:
- Clipped surrogate objective for stability
- Distributed training across multiple CPUs/GPUs
- Excellent for multi-agent environments

**Config Example**:
```python
config = {
    'state_encoder': {'type': 'feature'},
    'learning_rate': 3e-4,
    'gamma': 0.99,
    'num_workers': 2,  # Parallel rollout workers
    'clip_param': 0.2,
}
```

### 2. TorchRL SAC Agent

**Algorithm**: Soft Actor-Critic
**Best For**: Sample efficiency, exploration
**Key Features**:
- Off-policy learning (efficient data usage)
- Maximum entropy principle for exploration
- Stable training with soft target updates

**Config Example**:
```python
config = {
    'state_encoder': {'type': 'feature'},
    'learning_rate': 3e-4,
    'gamma': 0.99,
    'alpha': 0.2,  # Entropy temperature
    'batch_size': 256,
}
```

### 3. LangGraph LLM Agent

**Algorithm**: LLM Reasoning + RL Adaptation
**Best For**: Strategic decision-making, human-like play
**Key Features**:
- Uses GPT-4/Claude for strategic reasoning
- LangGraph workflow management
- Learns strategy preferences from outcomes

**Config Example**:
```python
config = {
    'llm_provider': 'openai',  # or 'anthropic'
    'model_name': 'gpt-4-turbo-preview',
    'temperature': 0.7,
}
```

## ğŸ“Š Training Progress

The training CLI provides real-time progress tracking:

```
Training PPO Agent...
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 45% 45/100 ETA: 02:35

â•­â”€ PPO Agent - Episode 40/100 â”€â•®
â”‚ Metric            Value       â”‚
â”‚ Win Rate          23.5%       â”‚
â”‚ Avg Reward        12.34       â”‚
â”‚ Avg Episode Time  2.45s       â”‚
â”‚ ETA               14:32:15    â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
```

## ğŸ’¾ Checkpoints & Model Management

### Automatic Checkpointing

Models are automatically saved during training:

```
checkpoints/
â”œâ”€â”€ multi_rl_20260108_123456/
â”‚   â”œâ”€â”€ ppo_ep50/
â”‚   â”‚   â”œâ”€â”€ checkpoint_data
â”‚   â”‚   â””â”€â”€ metadata.json
â”‚   â”œâ”€â”€ ppo_final/
â”‚   â”œâ”€â”€ sac_ep50/
â”‚   â”œâ”€â”€ sac_final/
â”‚   â”œâ”€â”€ llm_ep50/
â”‚   â””â”€â”€ llm_final/
```

### Loading Trained Models

```python
from catan_rl.agents.rllib_ppo_agent import RLlibPPOAgent
from pathlib import Path

# Create agent
agent = RLlibPPOAgent(agent_id=0, config={})

# Load checkpoint
checkpoint_path = Path("checkpoints/multi_rl_XXX/ppo_final")
agent.load_checkpoint(checkpoint_path)

# Set to evaluation mode
agent.set_training_mode(False)
```

## ğŸ® Training Modes

### 1. Single Agent Training

Train one agent against random opponents:

```bash
python train_agents.py --agent ppo --episodes 100
```

### 2. All Agents Training

Train all three agents sequentially:

```bash
python train_agents.py --agent all --episodes 100
```

### 3. Tournament Mode

After training, agents compete against each other:

```bash
python train_agents.py --agent all --episodes 100 --tournament 50
```

Tournament results:

```
â•”â• Tournament Results â•â•—
â•‘ Player â”‚ Agent â”‚ Wins â”‚ Win Rate â•‘
â•‘ 0      â”‚ PPO   â”‚ 15   â”‚ 30.0%    â•‘
â•‘ 1      â”‚ SAC   â”‚ 18   â”‚ 36.0%    â•‘
â•‘ 2      â”‚ LLM   â”‚ 12   â”‚ 24.0%    â•‘
â•‘ 3      â”‚ Rand  â”‚ 5    â”‚ 10.0%    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

## âš™ï¸ Configuration

### State Encoders

Choose how the game state is represented:

```python
# Feature-based encoding (90 features)
config = {'state_encoder': {'type': 'feature'}}

# Spatial CNN encoding (12Ã—7Ã—7 tensor)
config = {'state_encoder': {'type': 'spatial'}}
```

### Training Hyperparameters

Customize training behavior:

```python
config = {
    'learning_rate': 3e-4,  # Learning rate
    'gamma': 0.99,          # Discount factor
    'epsilon_start': 1.0,   # Initial exploration
    'epsilon_min': 0.1,     # Min exploration
    'epsilon_decay': 0.995, # Exploration decay
}
```

## ğŸ¯ Best Practices

### For Fast Training
- Use PPO with multiple workers
- Smaller network architectures
- Fewer episodes (100-500)

### For Best Performance
- SAC with large replay buffer
- More training episodes (1000+)
- Tune hyperparameters

### For Human-like Play
- LLM agent with GPT-4
- Higher temperature (0.7-0.9)
- Learn from human game transcripts

## ğŸ”§ Troubleshooting

### Ray RLlib not working
```bash
# Reinstall Ray with RLlib
pip install --upgrade ray[rllib]
```

### TorchRL errors
```bash
# Install TorchRL and TensorDict
pip install torchrl tensordict
```

### LLM agent using fallback mode
```bash
# Set API key
export OPENAI_API_KEY="your-key-here"

# Verify key is loaded
python -c "import os; print(os.getenv('OPENAI_API_KEY'))"
```

### Out of memory
- Reduce batch size
- Decrease replay buffer size
- Use fewer parallel workers

## ğŸ“ˆ Performance Tips

### Training Speed
1. **Use GPU**: Set `use_gpu: true` in config
2. **Parallel Workers**: Increase `num_workers` for PPO
3. **Smaller Batches**: Reduce `batch_size` for faster updates

### Sample Efficiency
1. **Use SAC**: Best sample efficiency
2. **Larger Replay Buffer**: More data to learn from
3. **Lower Learning Rate**: More stable learning

### Exploration
1. **Tune Entropy**: Higher alpha for SAC = more exploration
2. **Epsilon Schedule**: Decay epsilon slowly
3. **Diverse Training**: Mix opponent types

## ğŸ“ Next Steps

### After Training

1. **Evaluate Agents**: Run tournament mode
2. **Play Against AI**: Use web interface with trained agents
3. **Fine-tune**: Adjust hyperparameters based on results
4. **Extend**: Add new agent architectures

### Advanced Usage

1. **Custom Agents**: Implement `BaseRLAgent` interface
2. **Curriculum Learning**: Train against progressively harder opponents
3. **Transfer Learning**: Use pre-trained models
4. **Multi-agent Self-Play**: Agents train against each other

## ğŸ“š Architecture Overview

```
train_agents.py                    # Main training CLI
â”œâ”€â”€ TrainingSession               # Training orchestration
â””â”€â”€ BaseRLAgent implementations
    â”œâ”€â”€ RLlibPPOAgent            # Ray RLlib wrapper
    â”œâ”€â”€ TorchRLSACAgent          # TorchRL wrapper
    â””â”€â”€ LangGraphLLMAgent        # LangGraph wrapper

catan_rl/agents/
â”œâ”€â”€ base_rl_agent.py             # Common interface
â”œâ”€â”€ rllib_ppo_agent.py           # PPO implementation
â”œâ”€â”€ torchrl_sac_agent.py         # SAC implementation
â””â”€â”€ langgraph_llm_agent.py       # LLM implementation
```

## ğŸŒŸ Features

âœ… Beautiful CLI with progress bars
âœ… Real-time training metrics
âœ… Automatic checkpointing
âœ… Tournament mode
âœ… Multiple RL frameworks
âœ… Configurable player positions
âœ… Easy model loading
âœ… Comprehensive logging

## ğŸ“– References

- [Ray RLlib Documentation](https://docs.ray.io/en/latest/rllib/)
- [TorchRL Documentation](https://pytorch.org/rl/)
- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)
- [PPO Paper](https://arxiv.org/abs/1707.06347)
- [SAC Paper](https://arxiv.org/abs/1801.01290)

---

**Built with â¤ï¸ for strategic AI research**
